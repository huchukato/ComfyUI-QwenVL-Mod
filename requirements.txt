# ComfyUI-QwenVL requirements
# Note: Qwen3-VL models require transformers >= 4.57.0

transformers
torch
huggingface-hub
hf_xet
psutil
numpy
Pillow
opencv-python
bitsandbytes
accelerate

# For running GGUF models with vision support
# The [server] extra installs necessary components for multimodal models
# llama-cpp-python[server]

# Optional: Flash Attention 2 for 2-3x performance boost (RTX 20+)
# flash-attn>=2.0.0  # Uncomment for Flash Attention 2 support
